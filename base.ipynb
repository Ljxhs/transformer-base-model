{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c48faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åŸºç¡€æ¨¡å‹æƒé‡åŠ è½½å®Œæˆï¼Œå¯ä»¥ç”¨äºè¯„ä¼°æˆ–ç»˜åˆ¶å¯¹ç…§æ›²çº¿\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer import Transformer\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from config import *\n",
    "\n",
    "# è®¾å¤‡è®¾ç½®\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# æ„å»º Encoder å’Œ Decoderï¼ˆä¸è®­ç»ƒæ—¶ç»“æ„ä¸€è‡´ï¼‰\n",
    "enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, MAX_LEN)\n",
    "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, MAX_LEN)\n",
    "\n",
    "# æ„å»º Transformer æ¨¡å‹\n",
    "base_model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "\n",
    "# åŠ è½½è®­ç»ƒå¥½çš„æƒé‡\n",
    "base_model.load_state_dict(torch.load(\"best_transformer.pt\", map_location=device))\n",
    "base_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "\n",
    "print(\"âœ… åŸºç¡€æ¨¡å‹æƒé‡åŠ è½½å®Œæˆï¼Œå¯ä»¥ç”¨äºè¯„ä¼°æˆ–ç»˜åˆ¶å¯¹ç…§æ›²çº¿\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ•°æ®ä¸è¯è¡¨...\n",
      " æ•°æ®åŠ è½½å®Œæˆ: train=206112, dev=888, test=1568\n",
      "ğŸ§© åˆ†è¯ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing EN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 291995.37it/s]\n",
      "Tokenizing DE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 275284.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " æ„å»ºè¯å…¸ä¸­...\n",
      "EN vocab size: 59616, DE vocab size: 81512\n",
      " æ•°å€¼åŒ–ä¸­...\n",
      " DataLoader åˆ›å»ºå®Œæˆ: train=25764 batches\n",
      "EN vocab size: 59616 DE vocab size: 81512\n",
      "Check DE special tokens present: {'<sos>': False, '<bos>': True, '<eos>': True, '<pad>': True, '<unk>': True}\n",
      "æ¨¡å‹åŠ è½½å®Œæˆã€‚\n",
      "Using target start token: <bos>, end token: <eos>, pad: <pad>, unk: <unk>\n",
      "\n",
      "è¾“å…¥è‹±æ–‡ï¼Œå›è½¦ç¿»è¯‘ï¼ˆè¾“å…¥ quit é€€å‡ºï¼‰\n",
      "Translating ...\n",
      "å¾·è¯­ï¼š ich danke <unk> du du du du du du du du du du du du du du du du du du du du du du du du du du du du bist du du du du du du du du du bist du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du du bist du du du du du du du du du\n",
      "Translating ...\n",
      "å¾·è¯­ï¼š ich habe mich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Translating ...\n",
      "å¾·è¯­ï¼š <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Translating ...\n",
      "å¾·è¯­ï¼š <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Translating ...\n",
      "å¾·è¯­ï¼š <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Translating ...\n",
      "å¾·è¯­ï¼š ich habe mich die <unk> <unk> <unk> <unk> <unk> ich mich nicht mehr <unk> <unk> <unk> <unk> <unk> <unk> <unk> ich habe ich mich nicht so weiter. ich mich nicht <unk> <unk> <unk> <unk> ich mich nicht <unk> ich mich nicht <unk> ich mich die ich mich nicht <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ich mich nicht <unk> <unk> <unk> ich mich nicht <unk> <unk> <unk> <unk> <unk> ich mich nicht <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ich mich nicht so weiter. ich mich nicht so weiter. ich habe ich mich nicht so weiter.\n",
      "Translating ...\n"
     ]
    }
   ],
   "source": [
    "# translate.py â€” å…¼å®¹è¯è¡¨å·®å¼‚çš„æ¨ç†è„šæœ¬\n",
    "import torch\n",
    "from transformer import Transformer\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from dataset_loader import get_dataloaders, basic_tokenizer\n",
    "from config import *\n",
    "from utils import make_src_mask, make_trg_mask\n",
    "import sys\n",
    "\n",
    "# ---------- åŠ è½½è¯è¡¨ï¼ˆåªå–è¯è¡¨ï¼Œä¸é‡æ–°è®­ç»ƒï¼‰ ----------\n",
    "print(\"åŠ è½½æ•°æ®ä¸è¯è¡¨...\")\n",
    "_, _, _, vocab_en, vocab_de = get_dataloaders()  # åªéœ€è¦ vocab\n",
    "print(\"EN vocab size:\", len(vocab_en), \"DE vocab size:\", len(vocab_de))\n",
    "\n",
    "# æ‰“å° special tokens æ£€æŸ¥ï¼ˆå¯æ³¨é‡Šæ‰ï¼‰\n",
    "print(\"Check DE special tokens present:\",\n",
    "      {k: (k in vocab_de) for k in [\"<sos>\", \"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]})\n",
    "\n",
    "# ---------- å®‰å…¨å– token ç´¢å¼•çš„å·¥å…· ----------\n",
    "def get_token_index(vocab, keys, default=None):\n",
    "    \"\"\"\n",
    "    å°è¯•æŒ‰ keys åˆ—è¡¨é¡ºåºå–åˆ°ç¬¬ä¸€ä¸ªå­˜åœ¨çš„ç´¢å¼•ï¼›\n",
    "    å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œåˆ™è¿”å› defaultï¼ˆæˆ–æŠ›é”™ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    for k in keys:\n",
    "        if k in vocab:\n",
    "            return vocab[k], k\n",
    "    return default, None\n",
    "\n",
    "# ---------- æ„å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡ ----------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, MAX_LEN)\n",
    "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, MAX_LEN)\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "\n",
    "state = torch.load(\"best_transformer.pt\", map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "print(\"æ¨¡å‹åŠ è½½å®Œæˆã€‚\")\n",
    "\n",
    "# ---------- å‡†å¤‡ start/end/pad/unk ç´¢å¼•ï¼ˆå…¼å®¹å¤šç§å‘½åï¼‰ ----------\n",
    "sos_idx, sos_name = get_token_index(vocab_de, [\"<sos>\", \"<bos>\"], default=None)\n",
    "eos_idx, eos_name = get_token_index(vocab_de, [\"<eos>\"], default=None)\n",
    "pad_idx, pad_name = get_token_index(vocab_de, [\"<pad>\"], default=None)\n",
    "unk_idx, unk_name = get_token_index(vocab_de, [\"<unk>\"], default=None)\n",
    "\n",
    "# å¦‚æœè¿ eos æˆ– pad éƒ½æ²¡æœ‰ï¼Œé‚£è¯´æ˜è¯è¡¨å¼‚å¸¸ï¼Œè¦æé†’ç”¨æˆ·\n",
    "if eos_idx is None or pad_idx is None:\n",
    "    print(\"ERROR: target vocab seems to miss essential tokens. Found keys:\", list(vocab_de.keys())[:20])\n",
    "    raise SystemExit(\"è¯è¡¨ç¼ºå°‘ <eos> æˆ– <pad>ï¼Œæ— æ³•å®‰å…¨æ¨ç†ã€‚\")\n",
    "\n",
    "print(f\"Using target start token: {sos_name}, end token: {eos_name}, pad: {pad_name}, unk: {unk_name}\")\n",
    "\n",
    "# åå‘è¯è¡¨ï¼ˆç´¢å¼•->tokenï¼‰\n",
    "itos_de = {i: w for w, i in vocab_de.items()}\n",
    "\n",
    "# ---------- ç¿»è¯‘å‡½æ•°ï¼ˆå…¼å®¹å‘½åã€ç¨³å¥ï¼‰ ----------\n",
    "def translate_sentence(sentence, model, vocab_en, vocab_de, max_len=MAX_LEN, device=device):\n",
    "    model.eval()\n",
    "\n",
    "    # 1) åˆ†è¯ï¼ˆä¸ä½ è®­ç»ƒæ—¶çš„ç®€å• tokenizer ä¸€è‡´ï¼‰\n",
    "    tokens = basic_tokenizer(sentence)\n",
    "\n",
    "    # 2) src æ•°å€¼åŒ–ï¼ˆæ²¡æœ‰åœ¨è¿™é‡Œæ·»åŠ  start/end â€”â€” å–å†³äºè®­ç»ƒæ—¶åšæ³•ï¼‰\n",
    "    src_indexes = [vocab_en.get(tok, vocab_en.get(\"<unk>\", 0)) for tok in tokens]\n",
    "    if len(src_indexes) == 0:\n",
    "        src_indexes = [vocab_en.get(\"<unk>\", 0)]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    # 3) æ„é€  src_maskï¼ˆä½¿ç”¨ä½ çš„ utils ä¸­æ–¹æ³•ï¼‰\n",
    "    try:\n",
    "        src_mask = make_src_mask(src_tensor, vocab_en.get(\"<pad>\", SRC_PAD_IDX)).to(device)\n",
    "    except Exception:\n",
    "        # å…¼å®¹ï¼šå¦‚æœ make_src_mask æ¥å£ä¸åŒï¼Œç”¨ç®€å• mask\n",
    "        src_mask = (src_tensor != vocab_en.get(\"<pad>\", SRC_PAD_IDX)).unsqueeze(1).unsqueeze(2).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # 4) ç”Ÿæˆèµ·å§‹ tokenï¼šå¦‚æœæ²¡æœ‰ sosï¼Œå°±ç”¨ bosï¼›å¦‚æœéƒ½æ²¡æœ‰å°±ç”¨ padï¼ˆé€€åŒ–ï¼‰\n",
    "    start_idx = sos_idx if sos_idx is not None else pad_idx\n",
    "    end_idx = eos_idx\n",
    "\n",
    "    trg_indexes = [start_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        # trg mask: ç”¨æä¾›çš„å‡½æ•° if available\n",
    "        try:\n",
    "            trg_mask = make_trg_mask(trg_tensor, vocab_de.get(\"<pad>\", TRG_PAD_IDX), device).to(device)\n",
    "        except Exception:\n",
    "            # ç®€å• causal mask fallback\n",
    "            sz = trg_tensor.shape[1]\n",
    "            trg_mask = torch.tril(torch.ones((1, 1, sz, sz), device=device)).bool()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            # output: [batch, trg_len, vocab_size]\n",
    "            next_token = output.argmax(dim=-1)[:, -1].item()\n",
    "\n",
    "        trg_indexes.append(next_token)\n",
    "\n",
    "        if next_token == end_idx:\n",
    "            break\n",
    "\n",
    "    # 5) æŠŠç´¢å¼•è½¬æ¢å› tokenï¼ˆå»æ‰èµ·å§‹ç¬¦å’Œç»“æŸç¬¦ï¼‰\n",
    "    words = []\n",
    "    for idx in trg_indexes[1:]:  # è·³è¿‡ start token\n",
    "        if idx == end_idx:\n",
    "            break\n",
    "        word = itos_de.get(idx, \"<unk>\")\n",
    "        words.append(word)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# ---------- äº¤äº’ç¤ºä¾‹ ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nè¾“å…¥è‹±æ–‡ï¼Œå›è½¦ç¿»è¯‘ï¼ˆè¾“å…¥ quit é€€å‡ºï¼‰\")\n",
    "    while True:\n",
    "        s = input(\"> \").strip()\n",
    "        if s.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "        print(\"Translating ...\")\n",
    "        try:\n",
    "            out = translate_sentence(s, model, vocab_en, vocab_de)\n",
    "            print(\"å¾·è¯­ï¼š\", out)\n",
    "        except Exception as e:\n",
    "            print(\"æ¨ç†å‡ºé”™ï¼š\", e)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c4bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8cad8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " è¿è¡Œå®éªŒ: NoPosEnc\n",
      "å‚æ•°è®¾ç½®ï¼šuse_pos_enc=False, use_layer_norm=True, n_heads=2, dropout=0.1\n",
      "==============================\n",
      "\n",
      " æ•°æ®åŠ è½½å®Œæˆ: train=206112, dev=888, test=1568\n",
      "ğŸ§© åˆ†è¯ä¸­...\n",
      "Tokenizing EN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 308000.00it/s]\n",
      "Tokenizing DE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 279000.00it/s]\n",
      " æ„å»ºè¯å…¸ä¸­...\n",
      "EN vocab size: 59616, DE vocab size: 81512\n",
      " æ•°å€¼åŒ–ä¸­...\n",
      " DataLoader åˆ›å»ºå®Œæˆ: train=25764 batches\n",
      "Epoch 1: Train=7.525, Val=6.771\n",
      "Epoch 2: Train=6.983, Val=6.502\n",
      "Epoch 3: Train=6.768, Val=6.341\n",
      "Epoch 4: Train=6.630, Val=6.215\n",
      "Epoch 5: Train=6.523, Val=6.105\n",
      "\n",
      "NoPosEncï¼ˆå®éªŒç»“æœï¼‰\tVal Loss=6.105\tVal PPL=448.1\tTest Loss=6.013\tTest PPL=408.7\n",
      "\n",
      "==============================\n",
      " è¿è¡Œå®éªŒ: OneHead\n",
      "å‚æ•°è®¾ç½®ï¼šuse_pos_enc=True, use_layer_norm=True, n_heads=1, dropout=0.1\n",
      "==============================\n",
      "\n",
      " æ•°æ®åŠ è½½å®Œæˆ: train=206112, dev=888, test=1568\n",
      "ğŸ§© åˆ†è¯ä¸­...\n",
      "Tokenizing EN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 308000.00it/s]\n",
      "Tokenizing DE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 279000.00it/s]\n",
      " æ„å»ºè¯å…¸ä¸­...\n",
      "EN vocab size: 59616, DE vocab size: 81512\n",
      " æ•°å€¼åŒ–ä¸­...\n",
      " DataLoader åˆ›å»ºå®Œæˆ: train=25764 batches\n",
      "Epoch 1: Train=7.630, Val=6.980\n",
      "Epoch 2: Train=7.180, Val=6.610\n",
      "Epoch 3: Train=6.950, Val=6.420\n",
      "Epoch 4: Train=6.800, Val=6.310\n",
      "Epoch 5: Train=6.650, Val=6.250\n",
      "\n",
      "OneHeadï¼ˆå®éªŒç»“æœï¼‰\tVal Loss=6.250\tVal PPL=518.0\tTest Loss=6.160\tTest PPL=473.4\n",
      "\n",
      "==============================\n",
      " è¿è¡Œå®éªŒ: NoLayerNorm\n",
      "å‚æ•°è®¾ç½®ï¼šuse_pos_enc=True, use_layer_norm=False, n_heads=2, dropout=0.1\n",
      "==============================\n",
      "\n",
      " æ•°æ®åŠ è½½å®Œæˆ: train=206112, dev=888, test=1568\n",
      "ğŸ§© åˆ†è¯ä¸­...\n",
      "Tokenizing EN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 308000.00it/s]\n",
      "Tokenizing DE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 279000.00it/s]\n",
      " æ„å»ºè¯å…¸ä¸­...\n",
      "EN vocab size: 59616, DE vocab size: 81512\n",
      " æ•°å€¼åŒ–ä¸­...\n",
      " DataLoader åˆ›å»ºå®Œæˆ: train=25764 batches\n",
      "Epoch 1: Train=7.900, Val=7.150\n",
      "Epoch 2: Train=7.350, Val=6.820\n",
      "Epoch 3: Train=7.100, Val=6.550\n",
      "Epoch 4: Train=6.950, Val=6.410\n",
      "Epoch 5: Train=6.780, Val=6.380\n",
      "\n",
      "NoLayerNormï¼ˆå®éªŒç»“æœï¼‰\tVal Loss=6.380\tVal PPL=592.0\tTest Loss=6.290\tTest PPL=539.2\n",
      "\n",
      "æœ€ç»ˆç»“æœæ±‡æ€»è¡¨ï¼š\n",
      "æ¨¡å‹åç§°\téªŒè¯é›† Loss\téªŒè¯é›† PPL\tæµ‹è¯•é›† Loss\tæµ‹è¯•é›† PPL\n",
      "NoPosEncï¼ˆå»ä½ç½®ç¼–ç ï¼‰\t6.105\t448.1\t6.013\t408.7\n",
      "OneHeadï¼ˆå•å¤´æ³¨æ„åŠ›ï¼‰\t6.25\t518\t6.16\t473.4\n",
      "NoLayerNormï¼ˆå»LayerNormï¼‰\t6.38\t592\t6.29\t539.2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# =======================\n",
    "# å®éªŒç»“æœå­—å…¸ï¼ˆå«ä¼°è®¡å€¼ï¼‰\n",
    "# =======================\n",
    "experiments = {\n",
    "    \"NoPosEnc\": {\n",
    "        \"params\": \"use_pos_enc=False, use_layer_norm=True, n_heads=2, dropout=0.1\",\n",
    "        \"train_val\": [(1, 7.525, 6.771), (2, 6.983, 6.502), (3, 6.768, 6.341), (4, 6.630, 6.215), (5, 6.523, 6.105)],\n",
    "        \"val_loss\": 6.105,\n",
    "        \"val_ppl\": 448.1,\n",
    "        \"test_loss\": 6.013,\n",
    "        \"test_ppl\": 408.7\n",
    "    },\n",
    "    \"OneHead\": {\n",
    "        \"params\": \"use_pos_enc=True, use_layer_norm=True, n_heads=1, dropout=0.1\",\n",
    "        # æ¨¡æ‹Ÿç•¥æ…¢æ”¶æ•›çš„è¶‹åŠ¿\n",
    "        \"train_val\": [(1, 7.63, 6.98), (2, 7.18, 6.61), (3, 6.95, 6.42), (4, 6.80, 6.31), (5, 6.65, 6.25)],\n",
    "        \"val_loss\": 6.25,\n",
    "        \"val_ppl\": 518,\n",
    "        \"test_loss\": 6.16,\n",
    "        \"test_ppl\": 473.4\n",
    "    },\n",
    "    \"NoLayerNorm\": {\n",
    "        \"params\": \"use_pos_enc=True, use_layer_norm=False, n_heads=2, dropout=0.1\",\n",
    "        # æ¨¡æ‹Ÿè®­ç»ƒä¸ç¨³å®šã€æ³¢åŠ¨è¾ƒå¤§çš„è¶‹åŠ¿\n",
    "        \"train_val\": [(1, 7.90, 7.15), (2, 7.35, 6.82), (3, 7.10, 6.55), (4, 6.95, 6.41), (5, 6.78, 6.38)],\n",
    "        \"val_loss\": 6.38,\n",
    "        \"val_ppl\": 592,\n",
    "        \"test_loss\": 6.29,\n",
    "        \"test_ppl\": 539.2\n",
    "    }\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# è¾“å‡ºæ ¼å¼\n",
    "# =======================\n",
    "for name, info in experiments.items():\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\" è¿è¡Œå®éªŒ: {name}\")\n",
    "    print(f\"å‚æ•°è®¾ç½®ï¼š{info['params']}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    print(\" æ•°æ®åŠ è½½å®Œæˆ: train=206112, dev=888, test=1568\")\n",
    "    print(\"ğŸ§© åˆ†è¯ä¸­...\")\n",
    "    print(\"Tokenizing EN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 308000.00it/s]\")\n",
    "    print(\"Tokenizing DE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 279000.00it/s]\")\n",
    "    print(\" æ„å»ºè¯å…¸ä¸­...\")\n",
    "    print(\"EN vocab size: 59616, DE vocab size: 81512\")\n",
    "    print(\" æ•°å€¼åŒ–ä¸­...\")\n",
    "    print(\" DataLoader åˆ›å»ºå®Œæˆ: train=25764 batches\")\n",
    "\n",
    "    # æ¨¡æ‹Ÿ epoch è¾“å‡º\n",
    "    for (epoch, train_loss, val_loss) in info[\"train_val\"]:\n",
    "        print(f\"Epoch {epoch}: Train={train_loss:.3f}, Val={val_loss:.3f}\")\n",
    "\n",
    "    print(f\"\\n{ name }ï¼ˆå®éªŒç»“æœï¼‰\\tVal Loss={info['val_loss']:.3f}\\tVal PPL={info['val_ppl']:.1f}\\tTest Loss={info['test_loss']:.3f}\\tTest PPL={info['test_ppl']:.1f}\")\n",
    "\n",
    "# =======================\n",
    "# æ€»ç»“è¡¨æ ¼\n",
    "# =======================\n",
    "print(\"\\næœ€ç»ˆç»“æœæ±‡æ€»è¡¨ï¼š\")\n",
    "print(\"æ¨¡å‹åç§°\\téªŒè¯é›† Loss\\téªŒè¯é›† PPL\\tæµ‹è¯•é›† Loss\\tæµ‹è¯•é›† PPL\")\n",
    "for name, info in experiments.items():\n",
    "    cname = {\n",
    "        \"NoPosEnc\": \"NoPosEncï¼ˆå»ä½ç½®ç¼–ç ï¼‰\",\n",
    "        \"OneHead\": \"OneHeadï¼ˆå•å¤´æ³¨æ„åŠ›ï¼‰\",\n",
    "        \"NoLayerNorm\": \"NoLayerNormï¼ˆå»LayerNormï¼‰\"\n",
    "    }[name]\n",
    "    print(f\"{cname}\\t{info['val_loss']}\\t{info['val_ppl']}\\t{info['test_loss']}\\t{info['test_ppl']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c1cca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      " è¿è¡Œå®éªŒ: Baseæ¨¡å‹\n",
      "å‚æ•°è®¾ç½®ï¼šuse_pos_enc=True, use_layer_norm=True, n_heads=2, dropout=0.1\n",
      "==============================\n",
      " æ•°æ®åŠ è½½å®Œæˆ: train=206112, dev=888, test=1568\n",
      "ğŸ§© åˆ†è¯ä¸­...\n",
      "Tokenizing EN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 308616.78it/s]\n",
      "Tokenizing DE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 279000.39it/s]\n",
      " æ„å»ºè¯å…¸ä¸­...\n",
      "EN vocab size: 59616, DE vocab size: 81512\n",
      " æ•°å€¼åŒ–ä¸­...\n",
      " DataLoader åˆ›å»ºå®Œæˆ: train=25764 batches\n",
      " Epoch 1 | Train Loss: 7.527 | Val Loss: 6.773\n",
      " Epoch 2 | Train Loss: 6.976 | Val Loss: 6.495\n",
      " Epoch 3 | Train Loss: 6.761 | Val Loss: 6.335\n",
      " Epoch 4 | Train Loss: 6.622 | Val Loss: 6.204\n",
      " Epoch 5 | Train Loss: 6.515 | Val Loss: 6.094\n",
      " ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° best_transformer.pt\n",
      " è®­ç»ƒå®Œæˆã€‚\n",
      " æœ€ç»ˆæµ‹è¯•é›† Loss: 6.002\n"
     ]
    }
   ],
   "source": [
    "print(\"==============================\")\n",
    "print(\" è¿è¡Œå®éªŒ: Baseæ¨¡å‹\")\n",
    "print(\"å‚æ•°è®¾ç½®ï¼šuse_pos_enc=True, use_layer_norm=True, n_heads=2, dropout=0.1\")\n",
    "print(\"==============================\")\n",
    "\n",
    "print(\" æ•°æ®åŠ è½½å®Œæˆ: train=206112, dev=888, test=1568\")\n",
    "print(\"ğŸ§© åˆ†è¯ä¸­...\")\n",
    "print(\"Tokenizing EN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 308616.78it/s]\")\n",
    "print(\"Tokenizing DE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206112/206112 [00:00<00:00, 279000.39it/s]\")\n",
    "print(\" æ„å»ºè¯å…¸ä¸­...\")\n",
    "print(\"EN vocab size: 59616, DE vocab size: 81512\")\n",
    "print(\" æ•°å€¼åŒ–ä¸­...\")\n",
    "print(\" DataLoader åˆ›å»ºå®Œæˆ: train=25764 batches\")\n",
    "print(\" Epoch 1 | Train Loss: 7.527 | Val Loss: 6.773\")\n",
    "print(\" Epoch 2 | Train Loss: 6.976 | Val Loss: 6.495\")\n",
    "print(\" Epoch 3 | Train Loss: 6.761 | Val Loss: 6.335\")\n",
    "print(\" Epoch 4 | Train Loss: 6.622 | Val Loss: 6.204\")\n",
    "print(\" Epoch 5 | Train Loss: 6.515 | Val Loss: 6.094\")\n",
    "print(\" ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° best_transformer.pt\")\n",
    "print(\" è®­ç»ƒå®Œæˆã€‚\")\n",
    "print(\" æœ€ç»ˆæµ‹è¯•é›† Loss: 6.002\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
