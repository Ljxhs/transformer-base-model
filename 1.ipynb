{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d9c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据...\n",
      "\n",
      "数据加载成功!\n",
      "训练集句子数量: 206112\n",
      "验证集句子数量: 888\n",
      "测试集句子数量: 1568\n",
      "\n",
      "--- 数据样本 ---\n",
      "训练 (EN): Thank you so much, Chris.\n",
      "训练 (DE): Vielen Dank, Chris.\n",
      "\n",
      "验证 (EN): Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.\n",
      "验证 (DE): Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "# --- 1. 定义文件路径 ---\n",
    "data_dir = 'e:\\\\研究生\\\\大模型\\\\data'\n",
    "train_en_file = os.path.join(data_dir, 'train.tags.en-de.en')\n",
    "train_de_file = os.path.join(data_dir, 'train.tags.en-de.de')\n",
    "dev_en_xml = os.path.join(data_dir, 'IWSLT17.TED.dev2010.en-de.en.xml')\n",
    "dev_de_xml = os.path.join(data_dir, 'IWSLT17.TED.dev2010.en-de.de.xml')\n",
    "test_en_xml = os.path.join(data_dir, 'IWSLT17.TED.tst2010.en-de.en.xml')\n",
    "test_de_xml = os.path.join(data_dir, 'IWSLT17.TED.tst2010.en-de.de.xml')\n",
    "\n",
    "# --- 2. 定义数据加载函数 ---\n",
    "\n",
    "# 用于解析XML文件 (dev/test sets)\n",
    "def parse_xml(file_path):\n",
    "    \"\"\"解析IWSLT的XML文件，提取句子\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        sentences = []\n",
    "        for seg in root.findall('.//seg'):\n",
    "            sentences.append(seg.text.strip() if seg.text else \"\")\n",
    "        return sentences\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"按行读取纯文本文件，并过滤掉XML标签行\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # 过滤掉以'<'开头的行，这些是元数据标签\n",
    "            sentences = [line.strip() for line in f if not line.strip().startswith('<')]\n",
    "        return sentences\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. 加载所有数据集 ---\n",
    "print(\"开始加载数据...\")\n",
    "\n",
    "# 加载训练数据\n",
    "train_en = read_text_file(train_en_file)\n",
    "train_de = read_text_file(train_de_file)\n",
    "\n",
    "# 加载验证数据\n",
    "dev_en = parse_xml(dev_en_xml)\n",
    "dev_de = parse_xml(dev_de_xml)\n",
    "\n",
    "# 加载测试数据\n",
    "test_en = parse_xml(test_en_xml)\n",
    "test_de = parse_xml(test_de_xml)\n",
    "\n",
    "# --- 4. 检查加载结果 ---\n",
    "if train_en and dev_en and test_en:\n",
    "    print(\"\\n数据加载成功!\")\n",
    "    print(f\"训练集句子数量: {len(train_en)}\")\n",
    "    print(f\"验证集句子数量: {len(dev_en)}\")\n",
    "    print(f\"测试集句子数量: {len(test_en)}\")\n",
    "\n",
    "    # 确保每个数据集的源语言和目标语言句子数量匹配\n",
    "    assert len(train_en) == len(train_de), \"训练集语言对数量不匹配!\"\n",
    "    assert len(dev_en) == len(dev_de), \"验证集语言对数量不匹配!\"\n",
    "    assert len(test_en) == len(test_de), \"测试集语言对数量不匹配!\"\n",
    "\n",
    "    print(\"\\n--- 数据样本 ---\")\n",
    "    print(\"训练 (EN):\", train_en[0])\n",
    "    print(\"训练 (DE):\", train_de[0])\n",
    "    print(\"\\n验证 (EN):\", dev_en[0])\n",
    "    print(\"验证 (DE):\", dev_de[0])\n",
    "else:\n",
    "    print(\"\\n数据加载失败。请检查文件路径和文件内容是否正确。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc00fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始使用基础方法进行分词...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing EN Train: 100%|██████████| 206112/206112 [00:00<00:00, 402397.35it/s]\n",
      "Tokenizing DE Train: 100%|██████████| 206112/206112 [00:00<00:00, 231723.92it/s]\n",
      "Tokenizing EN Dev: 100%|██████████| 888/888 [00:00<00:00, 2840.09it/s]\n",
      "Tokenizing DE Dev: 100%|██████████| 888/888 [00:00<00:00, 286019.19it/s]\n",
      "Tokenizing EN Test: 100%|██████████| 1568/1568 [00:00<00:00, 457348.31it/s]\n",
      "Tokenizing DE Test: 100%|██████████| 1568/1568 [00:00<00:00, 358499.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分词完成!\n",
      "--- 分词样本 ---\n",
      "原始 (EN): Thank you so much, Chris.\n",
      "分词后 (EN): ['thank', 'you', 'so', 'much,', 'chris.']\n",
      "\n",
      "开始手动构建词典...\n",
      "\n",
      "词典构建完成!\n",
      "德语词典大小: 81512\n",
      "英语词典大小: 59616\n",
      "\n",
      "--- 词典映射示例 (DE) ---\n",
      "'vielen' -> 4\n",
      "'dank' -> 21\n",
      "'unbekanntes_wort' -> 0\n",
      "\n",
      "--- 索引到词元示例 (EN) ---\n",
      "Index 0 -> <unk>\n",
      "Index 1 -> <pad>\n",
      "Index 100 -> little\n",
      "\n",
      "--- 文本数值化示例 ---\n",
      "原始分词句子 (EN): ['thank', 'you', 'so', 'much,', 'chris.']\n",
      "数值化后: [4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# --- 5. (替代方案) 文本分词 (不使用spaCy) ---\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n开始使用基础方法进行分词...\")\n",
    "\n",
    "# 简单的分词函数：按空格切分并转为小写\n",
    "def basic_tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# 对所有数据进行分词\n",
    "tokenized_train_en = [basic_tokenizer(sent) for sent in tqdm(train_en, desc=\"Tokenizing EN Train\")]\n",
    "tokenized_train_de = [basic_tokenizer(sent) for sent in tqdm(train_de, desc=\"Tokenizing DE Train\")]\n",
    "tokenized_dev_en = [basic_tokenizer(sent) for sent in tqdm(dev_en, desc=\"Tokenizing EN Dev\")]\n",
    "tokenized_dev_de = [basic_tokenizer(sent) for sent in tqdm(dev_de, desc=\"Tokenizing DE Dev\")]\n",
    "tokenized_test_en = [basic_tokenizer(sent) for sent in tqdm(test_en, desc=\"Tokenizing EN Test\")]\n",
    "tokenized_test_de = [basic_tokenizer(sent) for sent in tqdm(test_de, desc=\"Tokenizing DE Test\")]\n",
    "\n",
    "print(\"\\n分词完成!\")\n",
    "print(\"--- 分词样本 ---\")\n",
    "print(\"原始 (EN):\", train_en[0])\n",
    "print(\"分词后 (EN):\", tokenized_train_en[0])\n",
    "\n",
    "\n",
    "# --- 6. (替代方案) 构建词典 (不使用torchtext) ---\n",
    "\n",
    "print(\"\\n开始手动构建词典...\")\n",
    "\n",
    "# 定义特殊符号\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "def build_vocab(tokenized_sentences, min_freq):\n",
    "    word_counts = Counter()\n",
    "    for sentence in tokenized_sentences:\n",
    "        word_counts.update(sentence)\n",
    "    \n",
    "    # 过滤低频词\n",
    "    filtered_words = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    \n",
    "    # 创建词典\n",
    "    vocab = {word: i + len(special_symbols) for i, word in enumerate(filtered_words)}\n",
    "    \n",
    "    # 添加特殊符号\n",
    "    for i, symbol in enumerate(special_symbols):\n",
    "        vocab[symbol] = i\n",
    "        \n",
    "    return vocab, {i: word for word, i in vocab.items()} # 返回 word-to-index 和 index-to-word\n",
    "\n",
    "# 构建词典\n",
    "vocab_de, itos_de = build_vocab(tokenized_train_de, min_freq=2)\n",
    "vocab_en, itos_en = build_vocab(tokenized_train_en, min_freq=2)\n",
    "\n",
    "UNK_IDX_DE = vocab_de['<unk>']\n",
    "UNK_IDX_EN = vocab_en['<unk>']\n",
    "\n",
    "print(\"\\n词典构建完成!\")\n",
    "print(f\"德语词典大小: {len(vocab_de)}\")\n",
    "print(f\"英语词典大小: {len(vocab_en)}\")\n",
    "\n",
    "# --- 7. 检查词典 ---\n",
    "print(\"\\n--- 词典映射示例 (DE) ---\")\n",
    "print(\"'vielen' ->\", vocab_de.get('vielen', UNK_IDX_DE))\n",
    "print(\"'dank' ->\", vocab_de.get('dank', UNK_IDX_DE))\n",
    "print(\"'unbekanntes_wort' ->\", vocab_de.get('unbekanntes_wort', UNK_IDX_DE)) # 未知词\n",
    "\n",
    "print(\"\\n--- 索引到词元示例 (EN) ---\")\n",
    "print(\"Index 0 ->\", itos_en[0])\n",
    "print(\"Index 1 ->\", itos_en[1])\n",
    "print(\"Index 100 ->\", itos_en[100])\n",
    "\n",
    "# --- 8. 文本数值化 ---\n",
    "def numericalize(tokenized_sentence, vocab, unk_idx):\n",
    "    return [vocab.get(token, unk_idx) for token in tokenized_sentence]\n",
    "\n",
    "sentence_en = tokenized_train_en[0]\n",
    "numericalized_sentence = numericalize(sentence_en, vocab_en, UNK_IDX_EN)\n",
    "print(\"\\n--- 文本数值化示例 ---\")\n",
    "print(\"原始分词句子 (EN):\", sentence_en)\n",
    "print(\"数值化后:\", numericalized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb021d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数值化所有数据...\n",
      "数值化完成!\n",
      "训练集样本数: 206112\n",
      "验证集样本数: 888\n",
      "\n",
      "创建了 1611 个训练批次，每个批次大小为 128\n",
      "\n",
      "--- 单个批次数据形状 ---\n",
      "源语言批次形状: torch.Size([128, 75])\n",
      "目标语言批次形状: torch.Size([128, 64])\n",
      "\n",
      "源语言批次第一个样本 (填充后):\n",
      "tensor([   74,    34,   215,  2721,   879,    85,   108,  1233,    15,  1220,\n",
      "         1739,  1255,  4690, 17151,   958,    17,     0,   642,  8625,    35,\n",
      "           39,  1249,   852,    34,    45,  2435,     5,   269,    16,  3121,\n",
      "        44254,  5065,     9, 18012, 14142,  2725, 37901, 44255,    15,    33,\n",
      "           17, 28644,    35,    17,  1259,  7189,    34,    17, 42470,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1])\n",
      "目标语言批次第一个样本 (填充后):\n",
      "tensor([   11,   626,  4466,   934,    64,   386,  1871,  1340, 16001, 18960,\n",
      "           67,   110,     0,   495,    18,   990,   335,   386,    32,  3644,\n",
      "         1679,   905,   767, 20485,  7554,    28,  2948, 58145,   119,   560,\n",
      "           33,    31, 49377, 20741,     0, 64174, 47869,    28,   206,  1024,\n",
      "           58,  4893, 64175,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- 0. 确保 text_to_numerical 函数已定义 ---\n",
    "def text_to_numerical(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# --- 1. 定义 Dataset ---\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 将数值化后的列表转换为 Tensor\n",
    "        src_sample = torch.tensor(self.src_data[idx], dtype=torch.long)\n",
    "        trg_sample = torch.tensor(self.trg_data[idx], dtype=torch.long)\n",
    "        return {'english': src_sample, 'german': trg_sample} \n",
    "\n",
    "# --- 2. 数值化所有数据 ---\n",
    "\n",
    "\n",
    "print(\"开始数值化所有数据...\")\n",
    "# 英语（EN）使用 vocab_en\n",
    "numericalized_train_en = [text_to_numerical(tokens, vocab_en) for tokens in tokenized_train_en]\n",
    "numericalized_dev_en = [text_to_numerical(tokens, vocab_en) for tokens in tokenized_dev_en]\n",
    "numericalized_test_en = [text_to_numerical(tokens, vocab_en) for tokens in tokenized_test_en]\n",
    "\n",
    "# 德语（DE）使用 vocab_de\n",
    "numericalized_train_de = [text_to_numerical(tokens, vocab_de) for tokens in tokenized_train_de]\n",
    "numericalized_dev_de = [text_to_numerical(tokens, vocab_de) for tokens in tokenized_dev_de]\n",
    "numericalized_test_de = [text_to_numerical(tokens, vocab_de) for tokens in tokenized_test_de]\n",
    "print(\"数值化完成!\")\n",
    "\n",
    "\n",
    "MAX_LEN = 300 # 与超参数保持一致\n",
    "\n",
    "# 示例：在数值化后进行截断\n",
    "numericalized_train_en = [tokens[:MAX_LEN] for tokens in numericalized_train_en]\n",
    "numericalized_train_de = [tokens[:MAX_LEN] for tokens in numericalized_train_de]\n",
    "numericalized_dev_en = [tokens[:MAX_LEN] for tokens in numericalized_dev_en] # <-- 加上\n",
    "numericalized_dev_de = [tokens[:MAX_LEN] for tokens in numericalized_dev_de] # <-- 加上\n",
    "numericalized_test_en = [tokens[:MAX_LEN] for tokens in numericalized_test_en] # <-- 加上\n",
    "numericalized_test_de = [tokens[:MAX_LEN] for tokens in numericalized_test_de] # <-- 加上\n",
    "\n",
    "# --- 3. 创建 Dataset 实例 ---\n",
    "\n",
    "train_dataset = TranslationDataset(numericalized_train_en, numericalized_train_de) \n",
    "val_dataset = TranslationDataset(numericalized_dev_en, numericalized_dev_de)\n",
    "test_dataset = TranslationDataset(numericalized_test_en, numericalized_test_de)\n",
    "\n",
    "\n",
    "print(f\"训练集样本数: {len(train_dataset)}\")\n",
    "print(f\"验证集样本数: {len(val_dataset)}\")\n",
    "\n",
    "# --- 4. 定义 collate_fn 以处理批次数据 ---\n",
    "PAD_IDX_EN = vocab_en['<pad>']\n",
    "PAD_IDX_DE = vocab_de['<pad>']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch = [sample['english'] for sample in batch]\n",
    "    trg_batch = [sample['german'] for sample in batch]\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX_EN, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX_DE, batch_first=True)\n",
    "    \n",
    "    return {'english': src_batch, 'german': trg_batch}\n",
    "\n",
    "\n",
    "\n",
    "# --- 5. 创建 DataLoader 实例 ---\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\n创建了 {len(train_dataloader)} 个训练批次，每个批次大小为 {BATCH_SIZE}\")\n",
    "\n",
    "# --- 6. 检查一个批次的数据 (修正了解包方式) --- \n",
    "batch = next(iter(train_dataloader))\n",
    "src_batch = batch['english'] # 正确获取英文张量\n",
    "trg_batch = batch['german']  # 正确获取德文张量\n",
    "\n",
    "print(f\"\\n--- 单个批次数据形状 ---\")\n",
    "print(f\"源语言批次形状: {src_batch.shape}\")\n",
    "print(f\"目标语言批次形状: {trg_batch.shape}\")\n",
    "\n",
    "print(f\"\\n源语言批次第一个样本 (填充后):\\n{src_batch[0]}\")\n",
    "print(f\"目标语言批次第一个样本 (填充后):\\n{trg_batch[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90ff601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大 EN 索引: 59615\n",
      "词表大小 EN: 59616\n",
      "最大 DE 索引: 81511\n",
      "词表大小 DE: 81512\n",
      "The model has 61,184,616 trainable parameters\n",
      "Source shape: torch.Size([128, 72])\n",
      "Target shape: torch.Size([128, 58])\n",
      "Output shape: torch.Size([128, 57, 81512])\n",
      "Attention: None (not returned in this version)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "# import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 导入之前定义的模块\n",
    "from modules import MultiHeadAttention, PositionwiseFeedforward, PositionalEncoding, LayerNorm\n",
    "from encoder import EncoderLayer, Encoder\n",
    "from decoder import DecoderLayer, Decoder\n",
    "from transformer import Transformer\n",
    "from utils import make_src_mask, make_trg_mask\n",
    "\n",
    "\n",
    "# 定义超参数\n",
    "INPUT_DIM = len(vocab_en) # 源语言词汇表大小\n",
    "OUTPUT_DIM = len(vocab_de) # 目标语言词汇表大小\n",
    "HID_DIM = 256 # 嵌入维度和模型内部维度\n",
    "ENC_LAYERS = 3 # 编码器层数\n",
    "DEC_LAYERS = 3 # 解码器层数\n",
    "ENC_HEADS = 8 # 编码器多头注意力头数\n",
    "DEC_HEADS = 8 # 解码器多头注意力头数\n",
    "ENC_PF_DIM = 512 # 编码器前馈网络维度\n",
    "DEC_PF_DIM = 512 # 解码器前馈网络维度\n",
    "ENC_DROPOUT = 0.1 # 编码器 dropout\n",
    "DEC_DROPOUT = 0.1 # 解码器 dropout\n",
    "MAX_LEN = 300 # 新增：序列的最大长度，用于位置编码\n",
    "\n",
    "SRC_PAD_IDX = vocab_en['<pad>'] # 源语言填充符索引\n",
    "TRG_PAD_IDX = vocab_de['<pad>'] # 目标语言填充符索引\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 实例化编码器和解码器\n",
    "enc = Encoder(\n",
    "    INPUT_DIM,\n",
    "    HID_DIM,\n",
    "    ENC_LAYERS,\n",
    "    ENC_HEADS,\n",
    "    ENC_PF_DIM,\n",
    "    ENC_DROPOUT,\n",
    "    MAX_LEN, # 新增：传递 MAX_LEN\n",
    ")\n",
    "\n",
    "dec = Decoder(\n",
    "    OUTPUT_DIM,\n",
    "    HID_DIM,\n",
    "    DEC_LAYERS,\n",
    "    DEC_HEADS,\n",
    "    DEC_PF_DIM,\n",
    "    DEC_DROPOUT,\n",
    "    MAX_LEN, # 新增：传递 MAX_LEN\n",
    ")\n",
    "\n",
    "print(\"最大 EN 索引:\", max(max(seq) for seq in numericalized_train_en))\n",
    "print(\"词表大小 EN:\", len(vocab_en))\n",
    "print(\"最大 DE 索引:\", max(max(seq) for seq in numericalized_train_de))\n",
    "print(\"词表大小 DE:\", len(vocab_de))\n",
    "\n",
    "\n",
    "# 实例化 Transformer 模型\n",
    "model = Transformer(\n",
    "    enc,\n",
    "    dec,\n",
    "    SRC_PAD_IDX,\n",
    "    TRG_PAD_IDX,\n",
    "    device\n",
    ").to(device)\n",
    "\n",
    "# 打印模型参数数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# 简单的前向传播测试\n",
    "# 假设一个批次的数据\n",
    "# src = [batch size, src len]\n",
    "# trg = [batch size, trg len]\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "src = batch['english'].to(device)\n",
    "trg = batch['german'].to(device)\n",
    "\n",
    "print(f\"Source shape: {src.shape}\")\n",
    "print(f\"Target shape: {trg.shape}\")\n",
    "\n",
    "output, attention = model(src, trg[:, :-1])\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "if attention is not None:\n",
    "    print(f\"Attention shape: {attention.shape}\")\n",
    "else:\n",
    "    print(\"Attention: None (not returned in this version)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decaaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91479be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
